{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7DnxtuxNfucU"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import copy\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "from torch.optim.lr_scheduler import _LRScheduler\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torchsummary import summary\n",
        "\n",
        "from sklearn import manifold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SdJhBGPffucY"
      },
      "outputs": [],
      "source": [
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hF0hx84YfucZ",
        "outputId": "fd18679f-63e8-4551-ecf4-df9d91f6df4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to .data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:01<00:00, 101852351.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting .data/cifar-10-python.tar.gz to .data\n"
          ]
        }
      ],
      "source": [
        "ROOT = '.data'\n",
        "\n",
        "train_data = datasets.CIFAR10(root = ROOT, \n",
        "                              train = True, \n",
        "                              download = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using transforms for data augmentation. Flip, crop and rotation is used to increase data diversity. Color channels are normalized using mean and standard deviation"
      ],
      "metadata": {
        "id": "KtqU-SA3-4Ei"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QACunP_8fuca"
      },
      "outputs": [],
      "source": [
        "means = train_data.data.mean(axis = (0,1,2)) / 255\n",
        "stds = train_data.data.std(axis = (0,1,2)) / 255\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "                           transforms.RandomRotation(5),\n",
        "                           transforms.RandomHorizontalFlip(0.5),\n",
        "                           transforms.RandomVerticalFlip(0.5),\n",
        "                           transforms.RandomCrop(32, padding = 2),\n",
        "                           transforms.ToTensor(),\n",
        "                           transforms.Normalize(mean = means, \n",
        "                                                std = stds)\n",
        "                       ])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "                           transforms.ToTensor(),\n",
        "                           transforms.Normalize(mean = means, \n",
        "                                                std = stds)\n",
        "                       ])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting data into, Training, validation, and testing sets"
      ],
      "metadata": {
        "id": "mZT5E6OC_HzW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJYnx2a0fucb",
        "outputId": "e03e297f-f4c2-4f17-8e58-2ed9caad6fd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "train_data = datasets.CIFAR10(ROOT, \n",
        "                              train = True, \n",
        "                              download = True, \n",
        "                              transform = train_transforms)\n",
        "\n",
        "test_data = datasets.CIFAR10(ROOT, \n",
        "                             train = False, \n",
        "                             download = True, \n",
        "                             transform = test_transforms)\n",
        "\n",
        "VALID_RATIO = 0.9\n",
        "\n",
        "n_train_examples = int(len(train_data) * VALID_RATIO)\n",
        "n_valid_examples = len(train_data) - n_train_examples\n",
        "\n",
        "train_data, valid_data = data.random_split(train_data, \n",
        "                                           [n_train_examples, n_valid_examples])\n",
        "\n",
        "valid_data = copy.deepcopy(valid_data)\n",
        "valid_data.dataset.transform = test_transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating data loaders for training, validation and test sets."
      ],
      "metadata": {
        "id": "XV8xh6vY_OjL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QU_NnNbnfucb",
        "outputId": "bf0b078f-572b-4a18-8e03-82c6400b279c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batches in train dataset: 176\n",
            "Batches in testing dataset: 40\n",
            "Number of images in train dataset: 45056\n",
            "Number of images in testing dataset: 10240\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 256\n",
        "\n",
        "train_iterator = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "valid_iterator = torch.utils.data.DataLoader(valid_data, batch_size = BATCH_SIZE, shuffle=False)\n",
        "test_iterator = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(\"Batches in train dataset: %d\" % (len(train_iterator)))\n",
        "print(\"Batches in testing dataset: %d\" % (len(test_iterator)))\n",
        "\n",
        "print(\"Number of images in train dataset: %d\" % (len(train_iterator) * BATCH_SIZE))\n",
        "print(\"Number of images in testing dataset: %d\" % (len(test_iterator) * BATCH_SIZE))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The custom model for resnet is created having 4,735,658 trainable parameters"
      ],
      "metadata": {
        "id": "F0gPiJ7p_TZN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VHfyO1V9fucc"
      },
      "outputs": [],
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = nn.functional.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = nn.functional.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_channels = 32\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.layer1 = self.make_layer(block, 32, num_blocks[0], stride=1)\n",
        "        self.layer2 = self.make_layer(block, 64, num_blocks[1], stride=2)\n",
        "        self.layer3 = self.make_layer(block, 128, num_blocks[2], stride=2)\n",
        "        self.layer4 = self.make_layer(block, 256, num_blocks[3], stride=2)\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(256, num_classes)\n",
        "\n",
        "    def make_layer(self, block, out_channels, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_channels, out_channels, stride))\n",
        "            self.in_channels = out_channels\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = nn.functional.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = self.avg_pool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3ELoAiNfucd",
        "outputId": "8a3cf458-fd4f-470e-f59c-5905d56555d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             864\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "            Conv2d-3           [-1, 32, 32, 32]           9,216\n",
            "       BatchNorm2d-4           [-1, 32, 32, 32]              64\n",
            "            Conv2d-5           [-1, 32, 32, 32]           9,216\n",
            "       BatchNorm2d-6           [-1, 32, 32, 32]              64\n",
            "     ResidualBlock-7           [-1, 32, 32, 32]               0\n",
            "            Conv2d-8           [-1, 32, 32, 32]           9,216\n",
            "       BatchNorm2d-9           [-1, 32, 32, 32]              64\n",
            "           Conv2d-10           [-1, 32, 32, 32]           9,216\n",
            "      BatchNorm2d-11           [-1, 32, 32, 32]              64\n",
            "    ResidualBlock-12           [-1, 32, 32, 32]               0\n",
            "           Conv2d-13           [-1, 32, 32, 32]           9,216\n",
            "      BatchNorm2d-14           [-1, 32, 32, 32]              64\n",
            "           Conv2d-15           [-1, 32, 32, 32]           9,216\n",
            "      BatchNorm2d-16           [-1, 32, 32, 32]              64\n",
            "    ResidualBlock-17           [-1, 32, 32, 32]               0\n",
            "           Conv2d-18           [-1, 64, 16, 16]          18,432\n",
            "      BatchNorm2d-19           [-1, 64, 16, 16]             128\n",
            "           Conv2d-20           [-1, 64, 16, 16]          36,864\n",
            "      BatchNorm2d-21           [-1, 64, 16, 16]             128\n",
            "           Conv2d-22           [-1, 64, 16, 16]           2,048\n",
            "      BatchNorm2d-23           [-1, 64, 16, 16]             128\n",
            "    ResidualBlock-24           [-1, 64, 16, 16]               0\n",
            "           Conv2d-25           [-1, 64, 16, 16]          36,864\n",
            "      BatchNorm2d-26           [-1, 64, 16, 16]             128\n",
            "           Conv2d-27           [-1, 64, 16, 16]          36,864\n",
            "      BatchNorm2d-28           [-1, 64, 16, 16]             128\n",
            "    ResidualBlock-29           [-1, 64, 16, 16]               0\n",
            "           Conv2d-30           [-1, 64, 16, 16]          36,864\n",
            "      BatchNorm2d-31           [-1, 64, 16, 16]             128\n",
            "           Conv2d-32           [-1, 64, 16, 16]          36,864\n",
            "      BatchNorm2d-33           [-1, 64, 16, 16]             128\n",
            "    ResidualBlock-34           [-1, 64, 16, 16]               0\n",
            "           Conv2d-35           [-1, 64, 16, 16]          36,864\n",
            "      BatchNorm2d-36           [-1, 64, 16, 16]             128\n",
            "           Conv2d-37           [-1, 64, 16, 16]          36,864\n",
            "      BatchNorm2d-38           [-1, 64, 16, 16]             128\n",
            "    ResidualBlock-39           [-1, 64, 16, 16]               0\n",
            "           Conv2d-40            [-1, 128, 8, 8]          73,728\n",
            "      BatchNorm2d-41            [-1, 128, 8, 8]             256\n",
            "           Conv2d-42            [-1, 128, 8, 8]         147,456\n",
            "      BatchNorm2d-43            [-1, 128, 8, 8]             256\n",
            "           Conv2d-44            [-1, 128, 8, 8]           8,192\n",
            "      BatchNorm2d-45            [-1, 128, 8, 8]             256\n",
            "    ResidualBlock-46            [-1, 128, 8, 8]               0\n",
            "           Conv2d-47            [-1, 128, 8, 8]         147,456\n",
            "      BatchNorm2d-48            [-1, 128, 8, 8]             256\n",
            "           Conv2d-49            [-1, 128, 8, 8]         147,456\n",
            "      BatchNorm2d-50            [-1, 128, 8, 8]             256\n",
            "    ResidualBlock-51            [-1, 128, 8, 8]               0\n",
            "           Conv2d-52            [-1, 128, 8, 8]         147,456\n",
            "      BatchNorm2d-53            [-1, 128, 8, 8]             256\n",
            "           Conv2d-54            [-1, 128, 8, 8]         147,456\n",
            "      BatchNorm2d-55            [-1, 128, 8, 8]             256\n",
            "    ResidualBlock-56            [-1, 128, 8, 8]               0\n",
            "           Conv2d-57            [-1, 128, 8, 8]         147,456\n",
            "      BatchNorm2d-58            [-1, 128, 8, 8]             256\n",
            "           Conv2d-59            [-1, 128, 8, 8]         147,456\n",
            "      BatchNorm2d-60            [-1, 128, 8, 8]             256\n",
            "    ResidualBlock-61            [-1, 128, 8, 8]               0\n",
            "           Conv2d-62            [-1, 256, 4, 4]         294,912\n",
            "      BatchNorm2d-63            [-1, 256, 4, 4]             512\n",
            "           Conv2d-64            [-1, 256, 4, 4]         589,824\n",
            "      BatchNorm2d-65            [-1, 256, 4, 4]             512\n",
            "           Conv2d-66            [-1, 256, 4, 4]          32,768\n",
            "      BatchNorm2d-67            [-1, 256, 4, 4]             512\n",
            "    ResidualBlock-68            [-1, 256, 4, 4]               0\n",
            "           Conv2d-69            [-1, 256, 4, 4]         589,824\n",
            "      BatchNorm2d-70            [-1, 256, 4, 4]             512\n",
            "           Conv2d-71            [-1, 256, 4, 4]         589,824\n",
            "      BatchNorm2d-72            [-1, 256, 4, 4]             512\n",
            "    ResidualBlock-73            [-1, 256, 4, 4]               0\n",
            "           Conv2d-74            [-1, 256, 4, 4]         589,824\n",
            "      BatchNorm2d-75            [-1, 256, 4, 4]             512\n",
            "           Conv2d-76            [-1, 256, 4, 4]         589,824\n",
            "      BatchNorm2d-77            [-1, 256, 4, 4]             512\n",
            "    ResidualBlock-78            [-1, 256, 4, 4]               0\n",
            "AdaptiveAvgPool2d-79            [-1, 256, 1, 1]               0\n",
            "           Linear-80                   [-1, 10]           2,570\n",
            "================================================================\n",
            "Total params: 4,735,658\n",
            "Trainable params: 4,735,658\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 8.91\n",
            "Params size (MB): 18.07\n",
            "Estimated Total Size (MB): 26.99\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = ResNet(ResidualBlock, [3, 4, 4, 3]).to(device)\n",
        "summary(model, (3,32,32))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "pd60AOEJfucd"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "criterion = criterion.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "TTtGdDLufucd"
      },
      "outputs": [],
      "source": [
        "def train(model, iterator, optimizer, criterion, device):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for (x, y) in iterator:\n",
        "        \n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "                \n",
        "        y_pred = model(x)\n",
        "        \n",
        "        loss = criterion(y_pred, y)\n",
        "        \n",
        "        pred = y_pred.argmax(1, keepdim = True)\n",
        "        correct = pred.eq(y.view_as(pred)).sum()\n",
        "        acc = correct.float() / y.shape[0]\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "\n",
        "\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "LBQdusXDfuce"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, iterator, criterion, device):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        \n",
        "        for (x, y) in iterator:\n",
        "\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            y_pred = model(x)\n",
        "\n",
        "            loss = criterion(y_pred, y)\n",
        "\n",
        "            pred = y_pred.argmax(1, keepdim = True)\n",
        "            correct = pred.eq(y.view_as(pred)).sum()\n",
        "            acc = correct.float() / y.shape[0]\n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Experiment: Training with Adam optimizer using 0.001 learning rate and weight_decay (L2 regularization) value of 0.0001"
      ],
      "metadata": {
        "id": "Px0TWJN6_osj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ResNet(ResidualBlock, [3, 4, 4, 3]).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4 )\n",
        "\n",
        "EPOCHS = 50\n",
        "\n",
        "best_validation_loss = float('inf')\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  \n",
        "  train_loss, train_accuracy = train(model, train_iterator, optimizer, criterion, device)\n",
        "  validation_loss, validation_accuracy = evaluate(model, valid_iterator, criterion, device)\n",
        "  \n",
        "  if validation_loss<best_validation_loss:\n",
        "    torch.save(model, \"best_model_new.pt\")\n",
        "    best_validation_loss=validation_loss \n",
        "\n",
        "  print (f\"Epoch: {epoch+1} \\ Training Loss={train_loss:.2f} \\ Training Accuracy: {train_accuracy:.2f} \")\n",
        "  print (f\"Epoch: {epoch+1} \\ Validation Loss={validation_loss:.2f} \\ Validation Accuracy: {validation_accuracy:.2f}\" ) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rT3SKUagWE4",
        "outputId": "acbc0d42-9b08-4fd2-eeb5-9ca77e557466"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 \\ Training Loss=1.61 \\ Training Accuracy: 0.40 \n",
            "Epoch: 1 \\ Validation Loss=1.39 \\ Validation Accuracy: 0.48\n",
            "Epoch: 2 \\ Training Loss=1.23 \\ Training Accuracy: 0.55 \n",
            "Epoch: 2 \\ Validation Loss=1.18 \\ Validation Accuracy: 0.58\n",
            "Epoch: 3 \\ Training Loss=1.03 \\ Training Accuracy: 0.63 \n",
            "Epoch: 3 \\ Validation Loss=1.00 \\ Validation Accuracy: 0.65\n",
            "Epoch: 4 \\ Training Loss=0.92 \\ Training Accuracy: 0.67 \n",
            "Epoch: 4 \\ Validation Loss=1.06 \\ Validation Accuracy: 0.63\n",
            "Epoch: 5 \\ Training Loss=0.83 \\ Training Accuracy: 0.70 \n",
            "Epoch: 5 \\ Validation Loss=0.99 \\ Validation Accuracy: 0.66\n",
            "Epoch: 6 \\ Training Loss=0.78 \\ Training Accuracy: 0.72 \n",
            "Epoch: 6 \\ Validation Loss=0.87 \\ Validation Accuracy: 0.70\n",
            "Epoch: 7 \\ Training Loss=0.73 \\ Training Accuracy: 0.74 \n",
            "Epoch: 7 \\ Validation Loss=0.93 \\ Validation Accuracy: 0.68\n",
            "Epoch: 8 \\ Training Loss=0.68 \\ Training Accuracy: 0.76 \n",
            "Epoch: 8 \\ Validation Loss=0.92 \\ Validation Accuracy: 0.69\n",
            "Epoch: 9 \\ Training Loss=0.64 \\ Training Accuracy: 0.77 \n",
            "Epoch: 9 \\ Validation Loss=0.82 \\ Validation Accuracy: 0.72\n",
            "Epoch: 10 \\ Training Loss=0.61 \\ Training Accuracy: 0.78 \n",
            "Epoch: 10 \\ Validation Loss=0.72 \\ Validation Accuracy: 0.75\n",
            "Epoch: 11 \\ Training Loss=0.58 \\ Training Accuracy: 0.80 \n",
            "Epoch: 11 \\ Validation Loss=0.74 \\ Validation Accuracy: 0.75\n",
            "Epoch: 12 \\ Training Loss=0.56 \\ Training Accuracy: 0.80 \n",
            "Epoch: 12 \\ Validation Loss=0.69 \\ Validation Accuracy: 0.77\n",
            "Epoch: 13 \\ Training Loss=0.53 \\ Training Accuracy: 0.81 \n",
            "Epoch: 13 \\ Validation Loss=0.62 \\ Validation Accuracy: 0.79\n",
            "Epoch: 14 \\ Training Loss=0.51 \\ Training Accuracy: 0.82 \n",
            "Epoch: 14 \\ Validation Loss=0.69 \\ Validation Accuracy: 0.77\n",
            "Epoch: 15 \\ Training Loss=0.50 \\ Training Accuracy: 0.83 \n",
            "Epoch: 15 \\ Validation Loss=0.71 \\ Validation Accuracy: 0.76\n",
            "Epoch: 16 \\ Training Loss=0.48 \\ Training Accuracy: 0.83 \n",
            "Epoch: 16 \\ Validation Loss=0.63 \\ Validation Accuracy: 0.78\n",
            "Epoch: 17 \\ Training Loss=0.46 \\ Training Accuracy: 0.84 \n",
            "Epoch: 17 \\ Validation Loss=0.61 \\ Validation Accuracy: 0.80\n",
            "Epoch: 18 \\ Training Loss=0.45 \\ Training Accuracy: 0.84 \n",
            "Epoch: 18 \\ Validation Loss=0.53 \\ Validation Accuracy: 0.82\n",
            "Epoch: 19 \\ Training Loss=0.44 \\ Training Accuracy: 0.84 \n",
            "Epoch: 19 \\ Validation Loss=0.56 \\ Validation Accuracy: 0.80\n",
            "Epoch: 20 \\ Training Loss=0.42 \\ Training Accuracy: 0.85 \n",
            "Epoch: 20 \\ Validation Loss=0.48 \\ Validation Accuracy: 0.83\n",
            "Epoch: 21 \\ Training Loss=0.41 \\ Training Accuracy: 0.86 \n",
            "Epoch: 21 \\ Validation Loss=0.54 \\ Validation Accuracy: 0.82\n",
            "Epoch: 22 \\ Training Loss=0.40 \\ Training Accuracy: 0.86 \n",
            "Epoch: 22 \\ Validation Loss=0.53 \\ Validation Accuracy: 0.82\n",
            "Epoch: 23 \\ Training Loss=0.39 \\ Training Accuracy: 0.86 \n",
            "Epoch: 23 \\ Validation Loss=0.48 \\ Validation Accuracy: 0.84\n",
            "Epoch: 24 \\ Training Loss=0.38 \\ Training Accuracy: 0.87 \n",
            "Epoch: 24 \\ Validation Loss=0.54 \\ Validation Accuracy: 0.82\n",
            "Epoch: 25 \\ Training Loss=0.36 \\ Training Accuracy: 0.87 \n",
            "Epoch: 25 \\ Validation Loss=0.62 \\ Validation Accuracy: 0.80\n",
            "Epoch: 26 \\ Training Loss=0.36 \\ Training Accuracy: 0.87 \n",
            "Epoch: 26 \\ Validation Loss=0.46 \\ Validation Accuracy: 0.84\n",
            "Epoch: 27 \\ Training Loss=0.36 \\ Training Accuracy: 0.87 \n",
            "Epoch: 27 \\ Validation Loss=0.50 \\ Validation Accuracy: 0.83\n",
            "Epoch: 28 \\ Training Loss=0.35 \\ Training Accuracy: 0.88 \n",
            "Epoch: 28 \\ Validation Loss=0.51 \\ Validation Accuracy: 0.83\n",
            "Epoch: 29 \\ Training Loss=0.34 \\ Training Accuracy: 0.88 \n",
            "Epoch: 29 \\ Validation Loss=0.57 \\ Validation Accuracy: 0.82\n",
            "Epoch: 30 \\ Training Loss=0.33 \\ Training Accuracy: 0.88 \n",
            "Epoch: 30 \\ Validation Loss=0.56 \\ Validation Accuracy: 0.81\n",
            "Epoch: 31 \\ Training Loss=0.33 \\ Training Accuracy: 0.89 \n",
            "Epoch: 31 \\ Validation Loss=0.48 \\ Validation Accuracy: 0.84\n",
            "Epoch: 32 \\ Training Loss=0.32 \\ Training Accuracy: 0.89 \n",
            "Epoch: 32 \\ Validation Loss=0.42 \\ Validation Accuracy: 0.86\n",
            "Epoch: 33 \\ Training Loss=0.31 \\ Training Accuracy: 0.89 \n",
            "Epoch: 33 \\ Validation Loss=0.51 \\ Validation Accuracy: 0.83\n",
            "Epoch: 34 \\ Training Loss=0.30 \\ Training Accuracy: 0.89 \n",
            "Epoch: 34 \\ Validation Loss=0.44 \\ Validation Accuracy: 0.86\n",
            "Epoch: 35 \\ Training Loss=0.31 \\ Training Accuracy: 0.89 \n",
            "Epoch: 35 \\ Validation Loss=0.45 \\ Validation Accuracy: 0.85\n",
            "Epoch: 36 \\ Training Loss=0.30 \\ Training Accuracy: 0.90 \n",
            "Epoch: 36 \\ Validation Loss=0.49 \\ Validation Accuracy: 0.85\n",
            "Epoch: 37 \\ Training Loss=0.29 \\ Training Accuracy: 0.90 \n",
            "Epoch: 37 \\ Validation Loss=0.45 \\ Validation Accuracy: 0.86\n",
            "Epoch: 38 \\ Training Loss=0.29 \\ Training Accuracy: 0.90 \n",
            "Epoch: 38 \\ Validation Loss=0.51 \\ Validation Accuracy: 0.84\n",
            "Epoch: 39 \\ Training Loss=0.28 \\ Training Accuracy: 0.90 \n",
            "Epoch: 39 \\ Validation Loss=0.48 \\ Validation Accuracy: 0.84\n",
            "Epoch: 40 \\ Training Loss=0.28 \\ Training Accuracy: 0.90 \n",
            "Epoch: 40 \\ Validation Loss=0.44 \\ Validation Accuracy: 0.86\n",
            "Epoch: 41 \\ Training Loss=0.28 \\ Training Accuracy: 0.90 \n",
            "Epoch: 41 \\ Validation Loss=0.41 \\ Validation Accuracy: 0.86\n",
            "Epoch: 42 \\ Training Loss=0.27 \\ Training Accuracy: 0.91 \n",
            "Epoch: 42 \\ Validation Loss=0.50 \\ Validation Accuracy: 0.84\n",
            "Epoch: 43 \\ Training Loss=0.26 \\ Training Accuracy: 0.91 \n",
            "Epoch: 43 \\ Validation Loss=0.48 \\ Validation Accuracy: 0.85\n",
            "Epoch: 44 \\ Training Loss=0.26 \\ Training Accuracy: 0.91 \n",
            "Epoch: 44 \\ Validation Loss=0.43 \\ Validation Accuracy: 0.86\n",
            "Epoch: 45 \\ Training Loss=0.25 \\ Training Accuracy: 0.91 \n",
            "Epoch: 45 \\ Validation Loss=0.41 \\ Validation Accuracy: 0.87\n",
            "Epoch: 46 \\ Training Loss=0.25 \\ Training Accuracy: 0.91 \n",
            "Epoch: 46 \\ Validation Loss=0.43 \\ Validation Accuracy: 0.86\n",
            "Epoch: 47 \\ Training Loss=0.25 \\ Training Accuracy: 0.91 \n",
            "Epoch: 47 \\ Validation Loss=0.50 \\ Validation Accuracy: 0.85\n",
            "Epoch: 48 \\ Training Loss=0.25 \\ Training Accuracy: 0.91 \n",
            "Epoch: 48 \\ Validation Loss=0.37 \\ Validation Accuracy: 0.88\n",
            "Epoch: 49 \\ Training Loss=0.24 \\ Training Accuracy: 0.92 \n",
            "Epoch: 49 \\ Validation Loss=0.43 \\ Validation Accuracy: 0.86\n",
            "Epoch: 50 \\ Training Loss=0.24 \\ Training Accuracy: 0.91 \n",
            "Epoch: 50 \\ Validation Loss=0.39 \\ Validation Accuracy: 0.87\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exp: Training with SGD optimizer using 0.1 learning rate and weight_decay (L2 regularization) value of 0.0001 and momentum=0.5. Also, we are using ReduceLROnPlateau to decrease learning rate as the training progresses"
      ],
      "metadata": {
        "id": "HwDISJkw_3M_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ResNet(ResidualBlock, [3, 4, 4, 3]).to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1, weight_decay=1e-4, momentum =0.5)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5)\n",
        "\n",
        "EPOCHS = 75\n",
        "\n",
        "best_validation_loss = float('inf')\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  \n",
        "  train_loss, train_accuracy = train(model, train_iterator, optimizer, criterion, device)\n",
        "  validation_loss, validation_accuracy = evaluate(model, valid_iterator, criterion, device)\n",
        "  scheduler.step(validation_loss)\n",
        "\n",
        "  if validation_loss<best_validation_loss:\n",
        "    torch.save(model, \"best_model_new.pt\")\n",
        "    best_validation_loss=validation_loss \n",
        "\n",
        "  print (f\"Epoch: {epoch+1} \\ Training Loss={train_loss:.2f} \\ Training Accuracy: {train_accuracy:.2f} \")\n",
        "  print (f\"Epoch: {epoch+1} \\ Validation Loss={validation_loss:.2f} \\ Validation Accuracy: {validation_accuracy:.2f}\" ) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqDPiQI0pvlK",
        "outputId": "fffccb6a-a60c-4936-b058-b9f65f030ed6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 \\ Training Loss=1.77 \\ Training Accuracy: 0.34 \n",
            "Epoch: 1 \\ Validation Loss=1.54 \\ Validation Accuracy: 0.44\n",
            "Epoch: 2 \\ Training Loss=1.35 \\ Training Accuracy: 0.50 \n",
            "Epoch: 2 \\ Validation Loss=1.73 \\ Validation Accuracy: 0.45\n",
            "Epoch: 3 \\ Training Loss=1.14 \\ Training Accuracy: 0.59 \n",
            "Epoch: 3 \\ Validation Loss=1.11 \\ Validation Accuracy: 0.60\n",
            "Epoch: 4 \\ Training Loss=1.00 \\ Training Accuracy: 0.64 \n",
            "Epoch: 4 \\ Validation Loss=1.20 \\ Validation Accuracy: 0.59\n",
            "Epoch: 5 \\ Training Loss=0.91 \\ Training Accuracy: 0.67 \n",
            "Epoch: 5 \\ Validation Loss=1.00 \\ Validation Accuracy: 0.65\n",
            "Epoch: 6 \\ Training Loss=0.83 \\ Training Accuracy: 0.70 \n",
            "Epoch: 6 \\ Validation Loss=0.98 \\ Validation Accuracy: 0.66\n",
            "Epoch: 7 \\ Training Loss=0.77 \\ Training Accuracy: 0.72 \n",
            "Epoch: 7 \\ Validation Loss=0.88 \\ Validation Accuracy: 0.70\n",
            "Epoch: 8 \\ Training Loss=0.71 \\ Training Accuracy: 0.75 \n",
            "Epoch: 8 \\ Validation Loss=0.86 \\ Validation Accuracy: 0.72\n",
            "Epoch: 9 \\ Training Loss=0.66 \\ Training Accuracy: 0.76 \n",
            "Epoch: 9 \\ Validation Loss=1.00 \\ Validation Accuracy: 0.69\n",
            "Epoch: 10 \\ Training Loss=0.63 \\ Training Accuracy: 0.78 \n",
            "Epoch: 10 \\ Validation Loss=0.74 \\ Validation Accuracy: 0.75\n",
            "Epoch: 11 \\ Training Loss=0.60 \\ Training Accuracy: 0.79 \n",
            "Epoch: 11 \\ Validation Loss=0.68 \\ Validation Accuracy: 0.77\n",
            "Epoch: 12 \\ Training Loss=0.56 \\ Training Accuracy: 0.80 \n",
            "Epoch: 12 \\ Validation Loss=0.73 \\ Validation Accuracy: 0.75\n",
            "Epoch: 13 \\ Training Loss=0.53 \\ Training Accuracy: 0.81 \n",
            "Epoch: 13 \\ Validation Loss=0.78 \\ Validation Accuracy: 0.74\n",
            "Epoch: 14 \\ Training Loss=0.51 \\ Training Accuracy: 0.82 \n",
            "Epoch: 14 \\ Validation Loss=0.58 \\ Validation Accuracy: 0.80\n",
            "Epoch: 15 \\ Training Loss=0.49 \\ Training Accuracy: 0.83 \n",
            "Epoch: 15 \\ Validation Loss=0.68 \\ Validation Accuracy: 0.78\n",
            "Epoch: 16 \\ Training Loss=0.47 \\ Training Accuracy: 0.84 \n",
            "Epoch: 16 \\ Validation Loss=0.63 \\ Validation Accuracy: 0.78\n",
            "Epoch: 17 \\ Training Loss=0.46 \\ Training Accuracy: 0.84 \n",
            "Epoch: 17 \\ Validation Loss=0.61 \\ Validation Accuracy: 0.80\n",
            "Epoch: 18 \\ Training Loss=0.44 \\ Training Accuracy: 0.85 \n",
            "Epoch: 18 \\ Validation Loss=0.63 \\ Validation Accuracy: 0.78\n",
            "Epoch: 19 \\ Training Loss=0.42 \\ Training Accuracy: 0.85 \n",
            "Epoch: 19 \\ Validation Loss=0.54 \\ Validation Accuracy: 0.82\n",
            "Epoch: 20 \\ Training Loss=0.41 \\ Training Accuracy: 0.86 \n",
            "Epoch: 20 \\ Validation Loss=0.52 \\ Validation Accuracy: 0.83\n",
            "Epoch: 21 \\ Training Loss=0.39 \\ Training Accuracy: 0.86 \n",
            "Epoch: 21 \\ Validation Loss=0.66 \\ Validation Accuracy: 0.78\n",
            "Epoch: 22 \\ Training Loss=0.38 \\ Training Accuracy: 0.87 \n",
            "Epoch: 22 \\ Validation Loss=0.54 \\ Validation Accuracy: 0.82\n",
            "Epoch: 23 \\ Training Loss=0.37 \\ Training Accuracy: 0.87 \n",
            "Epoch: 23 \\ Validation Loss=0.56 \\ Validation Accuracy: 0.82\n",
            "Epoch: 24 \\ Training Loss=0.36 \\ Training Accuracy: 0.87 \n",
            "Epoch: 24 \\ Validation Loss=0.55 \\ Validation Accuracy: 0.82\n",
            "Epoch: 25 \\ Training Loss=0.34 \\ Training Accuracy: 0.88 \n",
            "Epoch: 25 \\ Validation Loss=0.56 \\ Validation Accuracy: 0.82\n",
            "Epoch: 26 \\ Training Loss=0.33 \\ Training Accuracy: 0.88 \n",
            "Epoch: 26 \\ Validation Loss=0.63 \\ Validation Accuracy: 0.81\n",
            "Epoch: 27 \\ Training Loss=0.26 \\ Training Accuracy: 0.91 \n",
            "Epoch: 27 \\ Validation Loss=0.40 \\ Validation Accuracy: 0.87\n",
            "Epoch: 28 \\ Training Loss=0.23 \\ Training Accuracy: 0.92 \n",
            "Epoch: 28 \\ Validation Loss=0.39 \\ Validation Accuracy: 0.87\n",
            "Epoch: 29 \\ Training Loss=0.22 \\ Training Accuracy: 0.92 \n",
            "Epoch: 29 \\ Validation Loss=0.40 \\ Validation Accuracy: 0.87\n",
            "Epoch: 30 \\ Training Loss=0.21 \\ Training Accuracy: 0.92 \n",
            "Epoch: 30 \\ Validation Loss=0.39 \\ Validation Accuracy: 0.88\n",
            "Epoch: 31 \\ Training Loss=0.21 \\ Training Accuracy: 0.93 \n",
            "Epoch: 31 \\ Validation Loss=0.40 \\ Validation Accuracy: 0.88\n",
            "Epoch: 32 \\ Training Loss=0.21 \\ Training Accuracy: 0.93 \n",
            "Epoch: 32 \\ Validation Loss=0.39 \\ Validation Accuracy: 0.88\n",
            "Epoch: 33 \\ Training Loss=0.20 \\ Training Accuracy: 0.93 \n",
            "Epoch: 33 \\ Validation Loss=0.40 \\ Validation Accuracy: 0.87\n",
            "Epoch: 34 \\ Training Loss=0.19 \\ Training Accuracy: 0.93 \n",
            "Epoch: 34 \\ Validation Loss=0.40 \\ Validation Accuracy: 0.87\n",
            "Epoch: 35 \\ Training Loss=0.19 \\ Training Accuracy: 0.93 \n",
            "Epoch: 35 \\ Validation Loss=0.39 \\ Validation Accuracy: 0.88\n",
            "Epoch: 36 \\ Training Loss=0.18 \\ Training Accuracy: 0.94 \n",
            "Epoch: 36 \\ Validation Loss=0.39 \\ Validation Accuracy: 0.88\n",
            "Epoch: 37 \\ Training Loss=0.18 \\ Training Accuracy: 0.94 \n",
            "Epoch: 37 \\ Validation Loss=0.39 \\ Validation Accuracy: 0.88\n",
            "Epoch: 38 \\ Training Loss=0.18 \\ Training Accuracy: 0.94 \n",
            "Epoch: 38 \\ Validation Loss=0.39 \\ Validation Accuracy: 0.88\n",
            "Epoch: 39 \\ Training Loss=0.18 \\ Training Accuracy: 0.94 \n",
            "Epoch: 39 \\ Validation Loss=0.39 \\ Validation Accuracy: 0.88\n",
            "Epoch: 40 \\ Training Loss=0.18 \\ Training Accuracy: 0.94 \n",
            "Epoch: 40 \\ Validation Loss=0.39 \\ Validation Accuracy: 0.88\n",
            "Epoch: 41 \\ Training Loss=0.18 \\ Training Accuracy: 0.94 \n",
            "Epoch: 41 \\ Validation Loss=0.39 \\ Validation Accuracy: 0.88\n",
            "Epoch: 42 \\ Training Loss=0.18 \\ Training Accuracy: 0.94 \n",
            "Epoch: 42 \\ Validation Loss=0.39 \\ Validation Accuracy: 0.88\n",
            "Epoch: 43 \\ Training Loss=0.18 \\ Training Accuracy: 0.94 \n",
            "Epoch: 43 \\ Validation Loss=0.39 \\ Validation Accuracy: 0.88\n",
            "Epoch: 44 \\ Training Loss=0.18 \\ Training Accuracy: 0.94 \n",
            "Epoch: 44 \\ Validation Loss=0.39 \\ Validation Accuracy: 0.88\n",
            "Epoch: 45 \\ Training Loss=0.18 \\ Training Accuracy: 0.94 \n",
            "Epoch: 45 \\ Validation Loss=0.39 \\ Validation Accuracy: 0.88\n",
            "Epoch: 46 \\ Training Loss=0.18 \\ Training Accuracy: 0.94 \n",
            "Epoch: 46 \\ Validation Loss=0.39 \\ Validation Accuracy: 0.88\n",
            "Epoch: 47 \\ Training Loss=0.18 \\ Training Accuracy: 0.94 \n",
            "Epoch: 47 \\ Validation Loss=0.39 \\ Validation Accuracy: 0.88\n",
            "Epoch: 48 \\ Training Loss=0.18 \\ Training Accuracy: 0.94 \n",
            "Epoch: 48 \\ Validation Loss=0.39 \\ Validation Accuracy: 0.88\n",
            "Epoch: 49 \\ Training Loss=0.18 \\ Training Accuracy: 0.94 \n",
            "Epoch: 49 \\ Validation Loss=0.39 \\ Validation Accuracy: 0.88\n",
            "Epoch: 50 \\ Training Loss=0.18 \\ Training Accuracy: 0.94 \n",
            "Epoch: 50 \\ Validation Loss=0.39 \\ Validation Accuracy: 0.88\n",
            "Epoch: 51 \\ Training Loss=0.18 \\ Training Accuracy: 0.94 \n",
            "Epoch: 51 \\ Validation Loss=0.39 \\ Validation Accuracy: 0.88\n",
            "Epoch: 52 \\ Training Loss=0.18 \\ Training Accuracy: 0.94 \n",
            "Epoch: 52 \\ Validation Loss=0.39 \\ Validation Accuracy: 0.88\n",
            "Epoch: 53 \\ Training Loss=0.18 \\ Training Accuracy: 0.94 \n",
            "Epoch: 53 \\ Validation Loss=0.39 \\ Validation Accuracy: 0.88\n",
            "Epoch: 54 \\ Training Loss=0.18 \\ Training Accuracy: 0.94 \n",
            "Epoch: 54 \\ Validation Loss=0.39 \\ Validation Accuracy: 0.88\n",
            "Epoch: 55 \\ Training Loss=0.18 \\ Training Accuracy: 0.94 \n",
            "Epoch: 55 \\ Validation Loss=0.39 \\ Validation Accuracy: 0.88\n",
            "Epoch: 56 \\ Training Loss=0.18 \\ Training Accuracy: 0.94 \n",
            "Epoch: 56 \\ Validation Loss=0.39 \\ Validation Accuracy: 0.88\n",
            "Epoch: 57 \\ Training Loss=0.18 \\ Training Accuracy: 0.94 \n",
            "Epoch: 57 \\ Validation Loss=0.39 \\ Validation Accuracy: 0.88\n",
            "Epoch: 58 \\ Training Loss=0.18 \\ Training Accuracy: 0.94 \n",
            "Epoch: 58 \\ Validation Loss=0.39 \\ Validation Accuracy: 0.88\n",
            "Epoch: 59 \\ Training Loss=0.18 \\ Training Accuracy: 0.94 \n",
            "Epoch: 59 \\ Validation Loss=0.39 \\ Validation Accuracy: 0.88\n",
            "Epoch: 60 \\ Training Loss=0.18 \\ Training Accuracy: 0.94 \n",
            "Epoch: 60 \\ Validation Loss=0.39 \\ Validation Accuracy: 0.88\n",
            "Epoch: 61 \\ Training Loss=0.18 \\ Training Accuracy: 0.94 \n",
            "Epoch: 61 \\ Validation Loss=0.39 \\ Validation Accuracy: 0.88\n",
            "Epoch: 62 \\ Training Loss=0.18 \\ Training Accuracy: 0.94 \n",
            "Epoch: 62 \\ Validation Loss=0.39 \\ Validation Accuracy: 0.88\n",
            "Epoch: 63 \\ Training Loss=0.18 \\ Training Accuracy: 0.94 \n",
            "Epoch: 63 \\ Validation Loss=0.39 \\ Validation Accuracy: 0.88\n",
            "Epoch: 64 \\ Training Loss=0.18 \\ Training Accuracy: 0.94 \n",
            "Epoch: 64 \\ Validation Loss=0.39 \\ Validation Accuracy: 0.88\n",
            "Epoch: 65 \\ Training Loss=0.18 \\ Training Accuracy: 0.94 \n",
            "Epoch: 65 \\ Validation Loss=0.39 \\ Validation Accuracy: 0.88\n",
            "Epoch: 66 \\ Training Loss=0.18 \\ Training Accuracy: 0.94 \n",
            "Epoch: 66 \\ Validation Loss=0.39 \\ Validation Accuracy: 0.88\n",
            "Epoch: 67 \\ Training Loss=0.18 \\ Training Accuracy: 0.94 \n",
            "Epoch: 67 \\ Validation Loss=0.39 \\ Validation Accuracy: 0.88\n",
            "Epoch: 68 \\ Training Loss=0.18 \\ Training Accuracy: 0.94 \n",
            "Epoch: 68 \\ Validation Loss=0.39 \\ Validation Accuracy: 0.88\n",
            "Epoch: 69 \\ Training Loss=0.18 \\ Training Accuracy: 0.94 \n",
            "Epoch: 69 \\ Validation Loss=0.39 \\ Validation Accuracy: 0.88\n",
            "Epoch: 70 \\ Training Loss=0.18 \\ Training Accuracy: 0.94 \n",
            "Epoch: 70 \\ Validation Loss=0.39 \\ Validation Accuracy: 0.88\n",
            "Epoch: 71 \\ Training Loss=0.18 \\ Training Accuracy: 0.94 \n",
            "Epoch: 71 \\ Validation Loss=0.39 \\ Validation Accuracy: 0.88\n",
            "Epoch: 72 \\ Training Loss=0.18 \\ Training Accuracy: 0.94 \n",
            "Epoch: 72 \\ Validation Loss=0.39 \\ Validation Accuracy: 0.88\n",
            "Epoch: 73 \\ Training Loss=0.17 \\ Training Accuracy: 0.94 \n",
            "Epoch: 73 \\ Validation Loss=0.39 \\ Validation Accuracy: 0.88\n",
            "Epoch: 74 \\ Training Loss=0.18 \\ Training Accuracy: 0.94 \n",
            "Epoch: 74 \\ Validation Loss=0.39 \\ Validation Accuracy: 0.88\n",
            "Epoch: 75 \\ Training Loss=0.18 \\ Training Accuracy: 0.94 \n",
            "Epoch: 75 \\ Validation Loss=0.39 \\ Validation Accuracy: 0.88\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exp: Training with SGD optimizer using 0.01 learning rate and weight_decay (L2 regularization) value of 0.001 and momentum=0.9. Also, we are using CosineAnnealingLR to decrease learning rate as the training progresses"
      ],
      "metadata": {
        "id": "MRaITFXGANVp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UsFEP8pEfuce",
        "outputId": "2b99cfcb-3946-472c-e264-a4285b96100e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1 \\ Training Loss=1.71 \\ Training Accuracy: 0.35 \n",
            "Epoch: 1 \\ Validation Loss=1.60 \\ Validation Accuracy: 0.41\n",
            "Epoch: 2 \\ Training Loss=1.31 \\ Training Accuracy: 0.52 \n",
            "Epoch: 2 \\ Validation Loss=1.25 \\ Validation Accuracy: 0.56\n",
            "Epoch: 3 \\ Training Loss=1.09 \\ Training Accuracy: 0.61 \n",
            "Epoch: 3 \\ Validation Loss=1.32 \\ Validation Accuracy: 0.57\n",
            "Epoch: 4 \\ Training Loss=0.95 \\ Training Accuracy: 0.66 \n",
            "Epoch: 4 \\ Validation Loss=1.00 \\ Validation Accuracy: 0.66\n",
            "Epoch: 5 \\ Training Loss=0.85 \\ Training Accuracy: 0.69 \n",
            "Epoch: 5 \\ Validation Loss=0.86 \\ Validation Accuracy: 0.69\n",
            "Epoch: 6 \\ Training Loss=0.77 \\ Training Accuracy: 0.73 \n",
            "Epoch: 6 \\ Validation Loss=0.84 \\ Validation Accuracy: 0.71\n",
            "Epoch: 7 \\ Training Loss=0.72 \\ Training Accuracy: 0.74 \n",
            "Epoch: 7 \\ Validation Loss=0.74 \\ Validation Accuracy: 0.74\n",
            "Epoch: 8 \\ Training Loss=0.68 \\ Training Accuracy: 0.76 \n",
            "Epoch: 8 \\ Validation Loss=0.70 \\ Validation Accuracy: 0.76\n",
            "Epoch: 9 \\ Training Loss=0.64 \\ Training Accuracy: 0.78 \n",
            "Epoch: 9 \\ Validation Loss=0.71 \\ Validation Accuracy: 0.76\n",
            "Epoch: 10 \\ Training Loss=0.60 \\ Training Accuracy: 0.79 \n",
            "Epoch: 10 \\ Validation Loss=0.73 \\ Validation Accuracy: 0.75\n",
            "Epoch: 11 \\ Training Loss=0.57 \\ Training Accuracy: 0.80 \n",
            "Epoch: 11 \\ Validation Loss=0.71 \\ Validation Accuracy: 0.76\n",
            "Epoch: 12 \\ Training Loss=0.55 \\ Training Accuracy: 0.81 \n",
            "Epoch: 12 \\ Validation Loss=0.68 \\ Validation Accuracy: 0.76\n",
            "Epoch: 13 \\ Training Loss=0.53 \\ Training Accuracy: 0.81 \n",
            "Epoch: 13 \\ Validation Loss=0.62 \\ Validation Accuracy: 0.79\n",
            "Epoch: 14 \\ Training Loss=0.50 \\ Training Accuracy: 0.82 \n",
            "Epoch: 14 \\ Validation Loss=0.78 \\ Validation Accuracy: 0.75\n",
            "Epoch: 15 \\ Training Loss=0.49 \\ Training Accuracy: 0.83 \n",
            "Epoch: 15 \\ Validation Loss=0.69 \\ Validation Accuracy: 0.78\n",
            "Epoch: 16 \\ Training Loss=0.46 \\ Training Accuracy: 0.84 \n",
            "Epoch: 16 \\ Validation Loss=0.62 \\ Validation Accuracy: 0.79\n",
            "Epoch: 17 \\ Training Loss=0.44 \\ Training Accuracy: 0.84 \n",
            "Epoch: 17 \\ Validation Loss=0.54 \\ Validation Accuracy: 0.82\n",
            "Epoch: 18 \\ Training Loss=0.43 \\ Training Accuracy: 0.85 \n",
            "Epoch: 18 \\ Validation Loss=0.52 \\ Validation Accuracy: 0.82\n",
            "Epoch: 19 \\ Training Loss=0.42 \\ Training Accuracy: 0.85 \n",
            "Epoch: 19 \\ Validation Loss=0.57 \\ Validation Accuracy: 0.81\n",
            "Epoch: 20 \\ Training Loss=0.41 \\ Training Accuracy: 0.85 \n",
            "Epoch: 20 \\ Validation Loss=0.53 \\ Validation Accuracy: 0.82\n",
            "Epoch: 21 \\ Training Loss=0.39 \\ Training Accuracy: 0.86 \n",
            "Epoch: 21 \\ Validation Loss=0.56 \\ Validation Accuracy: 0.81\n",
            "Epoch: 22 \\ Training Loss=0.38 \\ Training Accuracy: 0.87 \n",
            "Epoch: 22 \\ Validation Loss=0.57 \\ Validation Accuracy: 0.81\n",
            "Epoch: 23 \\ Training Loss=0.37 \\ Training Accuracy: 0.87 \n",
            "Epoch: 23 \\ Validation Loss=0.48 \\ Validation Accuracy: 0.84\n",
            "Epoch: 24 \\ Training Loss=0.36 \\ Training Accuracy: 0.87 \n",
            "Epoch: 24 \\ Validation Loss=0.53 \\ Validation Accuracy: 0.82\n",
            "Epoch: 25 \\ Training Loss=0.35 \\ Training Accuracy: 0.88 \n",
            "Epoch: 25 \\ Validation Loss=0.57 \\ Validation Accuracy: 0.82\n",
            "Epoch: 26 \\ Training Loss=0.34 \\ Training Accuracy: 0.88 \n",
            "Epoch: 26 \\ Validation Loss=0.63 \\ Validation Accuracy: 0.80\n",
            "Epoch: 27 \\ Training Loss=0.33 \\ Training Accuracy: 0.88 \n",
            "Epoch: 27 \\ Validation Loss=0.45 \\ Validation Accuracy: 0.85\n",
            "Epoch: 28 \\ Training Loss=0.32 \\ Training Accuracy: 0.89 \n",
            "Epoch: 28 \\ Validation Loss=0.48 \\ Validation Accuracy: 0.84\n",
            "Epoch: 29 \\ Training Loss=0.31 \\ Training Accuracy: 0.89 \n",
            "Epoch: 29 \\ Validation Loss=0.49 \\ Validation Accuracy: 0.84\n",
            "Epoch: 30 \\ Training Loss=0.30 \\ Training Accuracy: 0.89 \n",
            "Epoch: 30 \\ Validation Loss=0.48 \\ Validation Accuracy: 0.84\n",
            "Epoch: 31 \\ Training Loss=0.29 \\ Training Accuracy: 0.90 \n",
            "Epoch: 31 \\ Validation Loss=0.63 \\ Validation Accuracy: 0.81\n",
            "Epoch: 32 \\ Training Loss=0.28 \\ Training Accuracy: 0.90 \n",
            "Epoch: 32 \\ Validation Loss=0.45 \\ Validation Accuracy: 0.85\n",
            "Epoch: 33 \\ Training Loss=0.28 \\ Training Accuracy: 0.90 \n",
            "Epoch: 33 \\ Validation Loss=0.48 \\ Validation Accuracy: 0.85\n",
            "Epoch: 34 \\ Training Loss=0.27 \\ Training Accuracy: 0.90 \n",
            "Epoch: 34 \\ Validation Loss=0.50 \\ Validation Accuracy: 0.84\n",
            "Epoch: 35 \\ Training Loss=0.26 \\ Training Accuracy: 0.91 \n",
            "Epoch: 35 \\ Validation Loss=0.45 \\ Validation Accuracy: 0.85\n",
            "Epoch: 36 \\ Training Loss=0.26 \\ Training Accuracy: 0.91 \n",
            "Epoch: 36 \\ Validation Loss=0.48 \\ Validation Accuracy: 0.85\n",
            "Epoch: 37 \\ Training Loss=0.25 \\ Training Accuracy: 0.91 \n",
            "Epoch: 37 \\ Validation Loss=0.45 \\ Validation Accuracy: 0.86\n",
            "Epoch: 38 \\ Training Loss=0.24 \\ Training Accuracy: 0.91 \n",
            "Epoch: 38 \\ Validation Loss=0.41 \\ Validation Accuracy: 0.87\n",
            "Epoch: 39 \\ Training Loss=0.24 \\ Training Accuracy: 0.92 \n",
            "Epoch: 39 \\ Validation Loss=0.44 \\ Validation Accuracy: 0.86\n",
            "Epoch: 40 \\ Training Loss=0.23 \\ Training Accuracy: 0.92 \n",
            "Epoch: 40 \\ Validation Loss=0.42 \\ Validation Accuracy: 0.86\n",
            "Epoch: 41 \\ Training Loss=0.23 \\ Training Accuracy: 0.92 \n",
            "Epoch: 41 \\ Validation Loss=0.47 \\ Validation Accuracy: 0.84\n",
            "Epoch: 42 \\ Training Loss=0.22 \\ Training Accuracy: 0.92 \n",
            "Epoch: 42 \\ Validation Loss=0.42 \\ Validation Accuracy: 0.86\n",
            "Epoch: 43 \\ Training Loss=0.21 \\ Training Accuracy: 0.93 \n",
            "Epoch: 43 \\ Validation Loss=0.44 \\ Validation Accuracy: 0.86\n",
            "Epoch: 44 \\ Training Loss=0.21 \\ Training Accuracy: 0.93 \n",
            "Epoch: 44 \\ Validation Loss=0.45 \\ Validation Accuracy: 0.86\n",
            "Epoch: 45 \\ Training Loss=0.20 \\ Training Accuracy: 0.93 \n",
            "Epoch: 45 \\ Validation Loss=0.47 \\ Validation Accuracy: 0.86\n",
            "Epoch: 46 \\ Training Loss=0.19 \\ Training Accuracy: 0.93 \n",
            "Epoch: 46 \\ Validation Loss=0.40 \\ Validation Accuracy: 0.87\n",
            "Epoch: 47 \\ Training Loss=0.19 \\ Training Accuracy: 0.93 \n",
            "Epoch: 47 \\ Validation Loss=0.45 \\ Validation Accuracy: 0.86\n",
            "Epoch: 48 \\ Training Loss=0.18 \\ Training Accuracy: 0.94 \n",
            "Epoch: 48 \\ Validation Loss=0.41 \\ Validation Accuracy: 0.87\n",
            "Epoch: 49 \\ Training Loss=0.18 \\ Training Accuracy: 0.94 \n",
            "Epoch: 49 \\ Validation Loss=0.46 \\ Validation Accuracy: 0.86\n",
            "Epoch: 50 \\ Training Loss=0.17 \\ Training Accuracy: 0.94 \n",
            "Epoch: 50 \\ Validation Loss=0.45 \\ Validation Accuracy: 0.86\n",
            "Epoch: 51 \\ Training Loss=0.17 \\ Training Accuracy: 0.94 \n",
            "Epoch: 51 \\ Validation Loss=0.39 \\ Validation Accuracy: 0.88\n",
            "Epoch: 52 \\ Training Loss=0.16 \\ Training Accuracy: 0.95 \n",
            "Epoch: 52 \\ Validation Loss=0.40 \\ Validation Accuracy: 0.88\n",
            "Epoch: 53 \\ Training Loss=0.15 \\ Training Accuracy: 0.95 \n",
            "Epoch: 53 \\ Validation Loss=0.44 \\ Validation Accuracy: 0.86\n",
            "Epoch: 54 \\ Training Loss=0.15 \\ Training Accuracy: 0.95 \n",
            "Epoch: 54 \\ Validation Loss=0.39 \\ Validation Accuracy: 0.88\n",
            "Epoch: 55 \\ Training Loss=0.14 \\ Training Accuracy: 0.95 \n",
            "Epoch: 55 \\ Validation Loss=0.40 \\ Validation Accuracy: 0.87\n",
            "Epoch: 56 \\ Training Loss=0.14 \\ Training Accuracy: 0.95 \n",
            "Epoch: 56 \\ Validation Loss=0.41 \\ Validation Accuracy: 0.88\n",
            "Epoch: 57 \\ Training Loss=0.13 \\ Training Accuracy: 0.95 \n",
            "Epoch: 57 \\ Validation Loss=0.40 \\ Validation Accuracy: 0.88\n",
            "Epoch: 58 \\ Training Loss=0.12 \\ Training Accuracy: 0.96 \n",
            "Epoch: 58 \\ Validation Loss=0.45 \\ Validation Accuracy: 0.87\n",
            "Epoch: 59 \\ Training Loss=0.12 \\ Training Accuracy: 0.96 \n",
            "Epoch: 59 \\ Validation Loss=0.42 \\ Validation Accuracy: 0.88\n",
            "Epoch: 60 \\ Training Loss=0.11 \\ Training Accuracy: 0.96 \n",
            "Epoch: 60 \\ Validation Loss=0.38 \\ Validation Accuracy: 0.89\n",
            "Epoch: 61 \\ Training Loss=0.11 \\ Training Accuracy: 0.96 \n",
            "Epoch: 61 \\ Validation Loss=0.44 \\ Validation Accuracy: 0.88\n",
            "Epoch: 62 \\ Training Loss=0.11 \\ Training Accuracy: 0.96 \n",
            "Epoch: 62 \\ Validation Loss=0.40 \\ Validation Accuracy: 0.88\n",
            "Epoch: 63 \\ Training Loss=0.10 \\ Training Accuracy: 0.96 \n",
            "Epoch: 63 \\ Validation Loss=0.38 \\ Validation Accuracy: 0.89\n",
            "Epoch: 64 \\ Training Loss=0.09 \\ Training Accuracy: 0.97 \n",
            "Epoch: 64 \\ Validation Loss=0.39 \\ Validation Accuracy: 0.89\n",
            "Epoch: 65 \\ Training Loss=0.09 \\ Training Accuracy: 0.97 \n",
            "Epoch: 65 \\ Validation Loss=0.39 \\ Validation Accuracy: 0.89\n",
            "Epoch: 66 \\ Training Loss=0.09 \\ Training Accuracy: 0.97 \n",
            "Epoch: 66 \\ Validation Loss=0.40 \\ Validation Accuracy: 0.88\n",
            "Epoch: 67 \\ Training Loss=0.08 \\ Training Accuracy: 0.97 \n",
            "Epoch: 67 \\ Validation Loss=0.39 \\ Validation Accuracy: 0.89\n",
            "Epoch: 68 \\ Training Loss=0.08 \\ Training Accuracy: 0.97 \n",
            "Epoch: 68 \\ Validation Loss=0.40 \\ Validation Accuracy: 0.89\n",
            "Epoch: 69 \\ Training Loss=0.08 \\ Training Accuracy: 0.97 \n",
            "Epoch: 69 \\ Validation Loss=0.37 \\ Validation Accuracy: 0.90\n",
            "Epoch: 70 \\ Training Loss=0.07 \\ Training Accuracy: 0.98 \n",
            "Epoch: 70 \\ Validation Loss=0.38 \\ Validation Accuracy: 0.89\n",
            "Epoch: 71 \\ Training Loss=0.07 \\ Training Accuracy: 0.98 \n",
            "Epoch: 71 \\ Validation Loss=0.37 \\ Validation Accuracy: 0.90\n",
            "Epoch: 72 \\ Training Loss=0.06 \\ Training Accuracy: 0.98 \n",
            "Epoch: 72 \\ Validation Loss=0.37 \\ Validation Accuracy: 0.90\n",
            "Epoch: 73 \\ Training Loss=0.06 \\ Training Accuracy: 0.98 \n",
            "Epoch: 73 \\ Validation Loss=0.37 \\ Validation Accuracy: 0.90\n",
            "Epoch: 74 \\ Training Loss=0.05 \\ Training Accuracy: 0.98 \n",
            "Epoch: 74 \\ Validation Loss=0.36 \\ Validation Accuracy: 0.90\n",
            "Epoch: 75 \\ Training Loss=0.05 \\ Training Accuracy: 0.98 \n",
            "Epoch: 75 \\ Validation Loss=0.37 \\ Validation Accuracy: 0.89\n",
            "Epoch: 76 \\ Training Loss=0.05 \\ Training Accuracy: 0.99 \n",
            "Epoch: 76 \\ Validation Loss=0.37 \\ Validation Accuracy: 0.90\n",
            "Epoch: 77 \\ Training Loss=0.04 \\ Training Accuracy: 0.99 \n",
            "Epoch: 77 \\ Validation Loss=0.38 \\ Validation Accuracy: 0.90\n",
            "Epoch: 78 \\ Training Loss=0.04 \\ Training Accuracy: 0.99 \n",
            "Epoch: 78 \\ Validation Loss=0.37 \\ Validation Accuracy: 0.89\n",
            "Epoch: 79 \\ Training Loss=0.04 \\ Training Accuracy: 0.99 \n",
            "Epoch: 79 \\ Validation Loss=0.35 \\ Validation Accuracy: 0.90\n",
            "Epoch: 80 \\ Training Loss=0.04 \\ Training Accuracy: 0.99 \n",
            "Epoch: 80 \\ Validation Loss=0.37 \\ Validation Accuracy: 0.90\n",
            "Epoch: 81 \\ Training Loss=0.04 \\ Training Accuracy: 0.99 \n",
            "Epoch: 81 \\ Validation Loss=0.37 \\ Validation Accuracy: 0.90\n",
            "Epoch: 82 \\ Training Loss=0.03 \\ Training Accuracy: 0.99 \n",
            "Epoch: 82 \\ Validation Loss=0.37 \\ Validation Accuracy: 0.90\n",
            "Epoch: 83 \\ Training Loss=0.03 \\ Training Accuracy: 0.99 \n",
            "Epoch: 83 \\ Validation Loss=0.37 \\ Validation Accuracy: 0.90\n",
            "Epoch: 84 \\ Training Loss=0.03 \\ Training Accuracy: 0.99 \n",
            "Epoch: 84 \\ Validation Loss=0.36 \\ Validation Accuracy: 0.90\n",
            "Epoch: 85 \\ Training Loss=0.03 \\ Training Accuracy: 0.99 \n",
            "Epoch: 85 \\ Validation Loss=0.36 \\ Validation Accuracy: 0.90\n",
            "Epoch: 86 \\ Training Loss=0.03 \\ Training Accuracy: 0.99 \n",
            "Epoch: 86 \\ Validation Loss=0.36 \\ Validation Accuracy: 0.90\n",
            "Epoch: 87 \\ Training Loss=0.03 \\ Training Accuracy: 0.99 \n",
            "Epoch: 87 \\ Validation Loss=0.36 \\ Validation Accuracy: 0.90\n",
            "Epoch: 88 \\ Training Loss=0.02 \\ Training Accuracy: 0.99 \n",
            "Epoch: 88 \\ Validation Loss=0.36 \\ Validation Accuracy: 0.90\n",
            "Epoch: 89 \\ Training Loss=0.02 \\ Training Accuracy: 0.99 \n",
            "Epoch: 89 \\ Validation Loss=0.36 \\ Validation Accuracy: 0.90\n",
            "Epoch: 90 \\ Training Loss=0.03 \\ Training Accuracy: 0.99 \n",
            "Epoch: 90 \\ Validation Loss=0.36 \\ Validation Accuracy: 0.90\n",
            "Epoch: 91 \\ Training Loss=0.02 \\ Training Accuracy: 0.99 \n",
            "Epoch: 91 \\ Validation Loss=0.36 \\ Validation Accuracy: 0.90\n",
            "Epoch: 92 \\ Training Loss=0.02 \\ Training Accuracy: 0.99 \n",
            "Epoch: 92 \\ Validation Loss=0.36 \\ Validation Accuracy: 0.90\n",
            "Epoch: 93 \\ Training Loss=0.02 \\ Training Accuracy: 0.99 \n",
            "Epoch: 93 \\ Validation Loss=0.36 \\ Validation Accuracy: 0.90\n",
            "Epoch: 94 \\ Training Loss=0.02 \\ Training Accuracy: 0.99 \n",
            "Epoch: 94 \\ Validation Loss=0.36 \\ Validation Accuracy: 0.90\n",
            "Epoch: 95 \\ Training Loss=0.02 \\ Training Accuracy: 0.99 \n",
            "Epoch: 95 \\ Validation Loss=0.36 \\ Validation Accuracy: 0.90\n",
            "Epoch: 96 \\ Training Loss=0.02 \\ Training Accuracy: 0.99 \n",
            "Epoch: 96 \\ Validation Loss=0.36 \\ Validation Accuracy: 0.90\n",
            "Epoch: 97 \\ Training Loss=0.02 \\ Training Accuracy: 1.00 \n",
            "Epoch: 97 \\ Validation Loss=0.36 \\ Validation Accuracy: 0.90\n",
            "Epoch: 98 \\ Training Loss=0.02 \\ Training Accuracy: 0.99 \n",
            "Epoch: 98 \\ Validation Loss=0.36 \\ Validation Accuracy: 0.90\n",
            "Epoch: 99 \\ Training Loss=0.02 \\ Training Accuracy: 1.00 \n",
            "Epoch: 99 \\ Validation Loss=0.36 \\ Validation Accuracy: 0.90\n",
            "Epoch: 100 \\ Training Loss=0.02 \\ Training Accuracy: 0.99 \n",
            "Epoch: 100 \\ Validation Loss=0.36 \\ Validation Accuracy: 0.90\n"
          ]
        }
      ],
      "source": [
        "model = ResNet(ResidualBlock, [3, 4, 4, 3]).to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1, weight_decay=1e-3, momentum=0.9)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n",
        "train_loss_history = []\n",
        "val_loss_history = []\n",
        "\n",
        "EPOCHS = 100\n",
        "\n",
        "best_validation_loss = float('inf')\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  \n",
        "  train_loss, train_accuracy = train(model, train_iterator, optimizer, criterion, device)\n",
        "  # train_loss, train_accuracy = train_with_scheduler(model, train_iterator, optimizer, scheduler, criterion, device)\n",
        "  scheduler.step()\n",
        "  validation_loss, validation_accuracy = evaluate(model, valid_iterator, criterion, device)\n",
        "  \n",
        "  if validation_loss<best_validation_loss:\n",
        "    torch.save(model, \"best_model_new.pt\")\n",
        "    best_validation_loss=validation_loss \n",
        "  \n",
        "  train_loss_history.append(train_loss)\n",
        "  val_loss_history.append(validation_loss)\n",
        "\n",
        "  print (f\"Epoch: {epoch+1} \\ Training Loss={train_loss:.2f} \\ Training Accuracy: {train_accuracy:.2f} \")\n",
        "  print (f\"Epoch: {epoch+1} \\ Validation Loss={validation_loss:.2f} \\ Validation Accuracy: {validation_accuracy:.2f}\" ) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijyD1sm2fuce",
        "outputId": "fbe44251-1150-4064-c51e-02f73fb89a82"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x302b52160>"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(train_loss_history, label=\"train loss\")\n",
        "plt.plot(val_loss_history, label=\"validation loss\")\n",
        "plt.xlabel('Epoch #')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ij1jluiBfucf",
        "outputId": "3a6a3356-3fa6-4e5b-8e7d-7afd6a516096"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.902734375\n"
          ]
        }
      ],
      "source": [
        "best_model = torch.load(\"best_model_new.pt\")\n",
        "test_loss, test_accuracy = evaluate(best_model, test_iterator, criterion, device)\n",
        "print(f\"Test Accuracy: {test_accuracy}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}